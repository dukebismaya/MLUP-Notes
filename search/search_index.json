{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ai-ml-notes-hub","title":"AI &amp; ML Notes Hub","text":"<p>Written by Bismaya.</p> <p>This site is a concise collection of structured Machine Learning notes. Use the search bar or open any note from the navigation to start reading. Each note can be marked complete at the bottom\u2014your progress is stored locally in your browser.</p>"},{"location":"#how-to-use","title":"How to Use","text":"<ol> <li>Pick a note from the left navigation (or use search).</li> <li>Read &amp; explore headings (TOC appears on longer notes).</li> <li>Click \u201cMark as Complete\u201d at the end of a note to track progress.</li> </ol>"},{"location":"#progress","title":"Progress","text":"<p>A mini progress widget appears below showing how many notes you\u2019ve completed.</p> <p>Local, lightweight, growing over time.</p>"},{"location":"curriculum/","title":"Machine Learning Curriculum Roadmap","text":"<p>A structured progression from fundamentals through modeling depth and into production &amp; governance. Use this page as an index to explore or track progress.</p> Overall Progress 0% <ul> <li> <p>\ud83c\udf93 Module 1 \u2013 Foundations Status: Complete Core concepts, problem framing, tools, linear modeling basics. Includes: Note 1\u20135  Start Module 1</p> </li> <li> <p>\ud83d\udcc8 Module 2 \u2013 Regression &amp; Dimensionality Reduction Status: Complete Regression theory, diagnostics, evaluation metrics, PCA, implementations. Includes: Note 6\u201313  Start Module 2</p> </li> <li> <p>\ud83e\uddee Module 3 \u2013 Advanced ML Status: In Planning Probabilistic methods, ensembles, feature selection vs extraction, interpretability. Includes (planned): Bayesian, Ensembles, SHAP, Regularization Paths  Placeholder</p> </li> <li> <p>\ud83c\udfed Module 4 \u2013 Production &amp; Ethics Status: In Planning MLOps lifecycle, deployment patterns, monitoring, drift, governance &amp; fairness. Includes (planned): Deployment, Monitoring, Fairness, Governance  Placeholder</p> </li> </ul>","tags":["curriculum","roadmap"]},{"location":"curriculum/#visual-progress-manual-tracking","title":"Visual Progress (Manual Tracking)","text":"<p>You can track personal completion using a simple checklist:</p> <ul> <li>[x] Module 1 \u2013 Foundations</li> <li>[x] Module 2 \u2013 Regression &amp; Dimensionality Reduction</li> <li>[ ] Module 3 \u2013 Advanced ML</li> <li>[ ] Module 4 \u2013 Production &amp; Ethics</li> </ul>","tags":["curriculum","roadmap"]},{"location":"curriculum/#suggested-learning-order","title":"Suggested Learning Order","text":"<ol> <li>Read all Module 1 to build terminology &amp; tooling base.</li> <li>Work through Module 2 while experimenting in notebooks (diagnostics + PCA).</li> <li>(Upcoming) TODO: Will be added by Bismaya (Module 3).</li> <li>(Upcoming) TODO: Will be added by Bismaya (Module 4).</li> </ol>","tags":["curriculum","roadmap"]},{"location":"curriculum/#extending-this-page","title":"Extending This Page","text":"<p>Planned future enhancements: - Dynamic progress (store in <code>localStorage</code>) - Per-note status badges (Complete / Draft / Planned) - Skill map heat visualization</p> <p>Have an idea? Add it as an Issue and link to this page.</p>","tags":["curriculum","roadmap"]},{"location":"notes/module3/","title":"Module 3 \u2013 Coming Soon","text":"<p>This module will expand into advanced machine learning paradigms.</p>","tags":["module","coming-soon"]},{"location":"notes/module3/#planned-topics-tentative","title":"Planned Topics (Tentative)","text":"<ul> <li>Probabilistic Modeling &amp; Bayesian Methods</li> <li>Generative Models</li> <li>Feature Selection vs Extraction (follow-up to PCA)</li> <li>Regularization Paths &amp; Model Selection</li> <li>Ensemble Methods (Bagging, Boosting, Stacking)</li> <li>Interpretability &amp; Explainability (SHAP, PDP, ICE)</li> </ul> <p>Status: Drafting phase. Content will roll out incrementally.</p>","tags":["module","coming-soon"]},{"location":"notes/module4/","title":"Module 4 \u2013 Coming Soon","text":"<p>This module will cover the full lifecycle and governance aspects of ML.</p>","tags":["module","coming-soon"]},{"location":"notes/module4/#planned-topics-tentative","title":"Planned Topics (Tentative)","text":"<ul> <li>MLOps Foundations (CI/CD for ML)</li> <li>Model Deployment Patterns (Batch, Online, Streaming)</li> <li>Monitoring &amp; Drift Detection</li> <li>Data &amp; Model Governance</li> <li>Fairness, Bias Mitigation &amp; Responsible AI</li> <li>Secure &amp; Privacy-Preserving ML</li> </ul> <p>Status: Planning phase. Outline subject to refinement.</p>","tags":["module","coming-soon"]},{"location":"notes/note1/","title":"Applications of Machine Learning in Different Fields","text":"<p>Machine Learning (ML) is widely applied across industries because of its ability to analyze data, learn patterns, and make predictions or decisions without being explicitly programmed.  </p>"},{"location":"notes/note1/#1-healthcare","title":"1. Healthcare","text":"<ul> <li>Disease Diagnosis \u2013 Detecting cancer, diabetes, and heart diseases from medical images or patient data.  </li> <li>Medical Imaging \u2013 ML models (CNNs) are used in MRI, CT, and X-ray interpretation.  </li> <li>Drug Discovery \u2013 Predicting how molecules interact to speed up drug design.  </li> <li>Personalized Medicine \u2013 Recommending treatments based on genetic data and patient history.  </li> <li>Predictive Analytics \u2013 Forecasting disease outbreaks and patient risks.  </li> </ul> <p>Key Insight</p> <p>ML is revolutionizing healthcare with faster diagnoses, personalized treatment, and predictive analytics.  </p>"},{"location":"notes/note1/#2-finance-banking","title":"2. Finance &amp; Banking","text":"<ul> <li>Fraud Detection \u2013 Identifying unusual transaction patterns.  </li> <li>Credit Scoring \u2013 Predicting loan repayment ability.  </li> <li>Algorithmic Trading \u2013 Using ML to make high-frequency stock trading decisions.  </li> <li>Customer Service \u2013 Chatbots for banking assistance.  </li> <li>Risk Management \u2013 Forecasting financial risks and market trends.  </li> </ul>"},{"location":"notes/note1/#3-retail-e-commerce","title":"3. Retail &amp; E-commerce","text":"<ul> <li>Recommendation Systems \u2013 Amazon, Flipkart, Netflix suggest products/movies.  </li> <li>Dynamic Pricing \u2013 Adjusting product prices based on demand and competition.  </li> <li>Customer Behavior Prediction \u2013 Analyzing browsing &amp; purchase history.  </li> <li>Inventory Management \u2013 Demand forecasting for stock control.  </li> <li>Chatbots &amp; Virtual Assistants \u2013 Handling customer queries automatically.  </li> </ul>"},{"location":"notes/note1/#4-education","title":"4. Education","text":"<ul> <li>Personalized Learning \u2013 Adaptive learning platforms (e.g., Duolingo) tailor lessons.  </li> <li>Automated Grading \u2013 AI-based essay and exam evaluation.  </li> <li>Early Warning Systems \u2013 Predicting student dropouts based on performance.  </li> <li>Virtual Teaching Assistants \u2013 Answering FAQs and guiding students.  </li> </ul>"},{"location":"notes/note1/#5-transportation-logistics","title":"5. Transportation &amp; Logistics","text":"<ul> <li>Self-Driving Cars \u2013 Tesla, Waymo use ML for navigation and object detection.  </li> <li>Traffic Prediction \u2013 Google Maps predicts congestion using ML.  </li> <li>Route Optimization \u2013 Delivery services (FedEx, UPS) reduce cost &amp; time.  </li> <li>Demand Forecasting \u2013 Predicting passenger demand for ride-sharing apps like Uber.  </li> </ul>"},{"location":"notes/note1/#6-manufacturing-industry-40","title":"6. Manufacturing &amp; Industry 4.0","text":"<ul> <li>Predictive Maintenance \u2013 Detecting when machines will fail.  </li> <li>Quality Control \u2013 Automated defect detection in production lines.  </li> <li>Supply Chain Optimization \u2013 Forecasting demand &amp; optimizing resources.  </li> <li>Robotics \u2013 ML-powered robots for automation.  </li> </ul>"},{"location":"notes/note1/#7-agriculture","title":"7. Agriculture","text":"<ul> <li>Crop Disease Detection \u2013 Image-based diagnosis of plant diseases.  </li> <li>Yield Prediction \u2013 Forecasting crop output based on soil &amp; weather data.  </li> <li>Smart Irrigation \u2013 Automated water usage prediction.  </li> <li>Drone Monitoring \u2013 Analyzing farm health from aerial images.  </li> </ul>"},{"location":"notes/note1/#8-cybersecurity","title":"8. Cybersecurity","text":"<ul> <li>Intrusion Detection \u2013 Identifying abnormal network traffic.  </li> <li>Malware Detection \u2013 ML models detect malicious software.  </li> <li>Phishing Detection \u2013 Recognizing fraudulent emails/websites.  </li> <li>User Authentication \u2013 Face/fingerprint recognition systems.  </li> </ul>"},{"location":"notes/note1/#9-entertainment-media","title":"9. Entertainment &amp; Media","text":"<ul> <li>Content Recommendation \u2013 Netflix, YouTube, Spotify.  </li> <li>Game AI \u2013 ML-driven opponents in video games.  </li> <li>Content Creation \u2013 Generating music, images, and videos with AI.  </li> <li>Fake News Detection \u2013 Identifying misinformation on social media.  </li> </ul>"},{"location":"notes/note1/#10-government-public-sector","title":"10. Government &amp; Public Sector","text":"<ul> <li>Crime Prediction \u2013 Identifying crime hotspots.  </li> <li>Smart Cities \u2013 Traffic, energy, and waste management optimization.  </li> <li>Defense &amp; Surveillance \u2013 Facial recognition, drone monitoring.  </li> <li>Disaster Management \u2013 Predicting floods, earthquakes, and natural calamities.  </li> </ul>"},{"location":"notes/note1/#in-short","title":"\u2705 In Short","text":"<ul> <li>Healthcare \u2192 Diagnosis &amp; drug discovery  </li> <li>Finance \u2192 Fraud detection &amp; trading  </li> <li>Retail \u2192 Recommendations &amp; pricing  </li> <li>Education \u2192 Personalized learning  </li> <li>Transport \u2192 Self-driving &amp; logistics  </li> <li>Manufacturing \u2192 Predictive maintenance  </li> <li>Agriculture \u2192 Crop monitoring  </li> <li>Cybersecurity \u2192 Threat detection  </li> <li>Entertainment \u2192 Content recommendations  </li> <li>Government \u2192 Smart cities &amp; crime prediction  </li> </ul>"},{"location":"notes/note10/","title":"note10","text":"<p>title: \"Multiple Linear Regression\" description: \"Definition, formula, OLS solution, assumptions, diagnostics, Python example, interpretation, and applications\" tags:     - regression     - linear-models     - supervised-learning     - fundamentals</p>"},{"location":"notes/note10/#multiple-linear-regression-mlr","title":"Multiple Linear Regression (MLR)","text":"<p>Multiple Linear Regression extends simple linear regression to model the relationship between a continuous target variable and two or more predictor variables.</p>"},{"location":"notes/note10/#1-mathematical-model","title":"1. Mathematical Model","text":"<p>\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon\\) Where: - \\(Y\\) = dependent (target) variable - \\(X_1, X_2, \\dots, X_p\\) = independent features - \\(\\beta_0\\) = intercept - \\(\\beta_i\\) = coefficient (marginal effect of feature \\(i\\) holding others constant) - \\(\\varepsilon\\) = error term (unexplained noise)</p>"},{"location":"notes/note10/#matrix-form","title":"Matrix Form","text":"<p>Let \\(X \\in \\mathbb{R}^{n \\times (p+1)}\\) with first column of ones (intercept), \\(\\beta \\in \\mathbb{R}^{p+1}\\), \\(y \\in \\mathbb{R}^n\\): \\(y = X\\beta + \\varepsilon\\)</p>"},{"location":"notes/note10/#ordinary-least-squares-ols-solution","title":"Ordinary Least Squares (OLS) Solution","text":"<p>Minimize \\(\\|y - X\\beta\\|_2^2\\). Closed-form (when \\(X^T X\\) invertible): \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\)</p> <p>In practice, libraries use QR decomposition / SVD for numerical stability.</p>"},{"location":"notes/note10/#2-core-assumptions","title":"2. Core Assumptions","text":"Assumption Description Violation Indicator Potential Remedies Linearity Expected value of \\(Y\\) is linear in coefficients Curvature in residual vs fitted Add transforms / interactions Independence Observations are independent Autocorrelation in residuals Use time-series models / cluster robust SE Homoscedasticity Constant error variance Funnel pattern, Breusch-Pagan p &lt; 0.05 Transform target, weighted / robust regression Normality (errors) Residuals ~ Normal (for inference) Heavy tails in Q-Q Larger sample CLT, transform, robust SE No Multicollinearity Predictors not redundant High VIF (&gt;5 or &gt;10) Drop / combine features, regularize <p>See earlier notes on residual diagnostics (note6) for plots &amp; tests.</p>"},{"location":"notes/note10/#3-python-example","title":"3. Python Example","text":"<p>Predict student exam score from hours studied and hours slept. Python<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndata = {\n        \"Hours_Studied\": [5, 10, 8, 12, 15],\n        \"Hours_Slept\":   [7,  6, 8,  5,  4],\n        \"Exam_Score\":    [75, 85, 82, 90, 95]\n}\ndf = pd.DataFrame(data)\n\nX = df[[\"Hours_Studied\", \"Hours_Slept\"]]\ny = df[\"Exam_Score\"]\n\nmodel = LinearRegression().fit(X, y)\npred = model.predict(X)\n\nprint(\"Intercept (\u03b20):\", model.intercept_)\nprint(\"Coefficients (\u03b21, \u03b22):\", model.coef_)\nprint(\"R\u00b2:\", r2_score(y, pred))\nprint(\"MSE:\", mean_squared_error(y, pred))\n\n# Adjusted R\u00b2\nn = len(y)\np = X.shape[1]\nr2 = r2_score(y, pred)\nadj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\nprint(\"Adjusted R\u00b2:\", adj_r2)\n\n# VIF Check (small sample just illustrative)\nX_with_const = np.column_stack([np.ones(len(X)), X.values])\nvif = []\nfor i in range(1, X_with_const.shape[1]):  # skip intercept column\n        vif_val = variance_inflation_factor(X_with_const, i)\n        vif.append((X.columns[i-1], vif_val))\nprint(\"VIF:\", vif)\n\n# Predict for a new student\nnew_student = np.array([[14, 6]])\nprint(\"Prediction (14 study hrs, 6 sleep hrs):\", model.predict(new_student)[0])\n</code></pre></p> <p>NOTE: Sample size is extremely small (n=5); metrics are not reliable\u2014example is pedagogical.</p>"},{"location":"notes/note10/#4-interpretation-of-results","title":"4. Interpretation of Results","text":"<ul> <li>Intercept (\\(\\beta_0\\)): Expected exam score if all predictors are 0 (may be extrapolation; interpret cautiously).</li> <li>Coefficient \\(\\beta_1\\) (Hours_Studied): Expected change in score per extra study hour holding sleep constant.</li> <li>Coefficient \\(\\beta_2\\) (Hours_Slept): Expected change in score per extra sleep hour holding study constant.</li> <li>\\(R^2\\) / Adjusted \\(R^2\\): Variance explained; adjusted penalizes complexity.</li> <li>VIF: Gauges multicollinearity; near 1 is good, high values flag redundancy.</li> </ul>"},{"location":"notes/note10/#5-common-applications","title":"5. Common Applications","text":"<ul> <li>House price prediction (size, rooms, location, age)</li> <li>Sales forecasting (ad spend, seasonality, discounting)</li> <li>Salary estimation (experience, education, role features)</li> <li>Medical outcomes (biomarkers, lifestyle indicators)</li> <li>Energy consumption (weather, occupancy, equipment status)</li> </ul>"},{"location":"notes/note10/#6-pitfalls-remedies","title":"6. Pitfalls &amp; Remedies","text":"Issue Symptom Strategy Multicollinearity Unstable coefficients, high VIF Drop / combine features, Ridge regression Omitted variable bias Residual structure, low \\(R^2\\) Add relevant predictors, domain consultation Overfitting High train \\(R^2\\), low test \\(R^2\\) Cross-validation, regularization Heteroscedasticity Patterned residual spread Transform target, robust SE, WLS Non-linearity Curved residual patterns Add interactions, polynomial terms, splines"},{"location":"notes/note10/#7-relation-to-other-linear-models","title":"7. Relation to Other Linear Models","text":"<ul> <li>Simple Linear Regression: Special case with \\(p=1\\).</li> <li>Polynomial Regression: Linear model on expanded (non-linear) basis.</li> <li>Regularized Models: Ridge/Lasso mitigate multicollinearity &amp; overfitting.</li> <li>Generalized Linear Models (GLMs): Extend linear predictor to non-Gaussian targets via link functions.</li> </ul>"},{"location":"notes/note10/#8-summary","title":"8. Summary","text":"<p>Multiple Linear Regression provides a transparent baseline for modeling continuous outcomes with multiple predictors. Its validity depends on assumptions; diagnostics (see note6) and careful feature engineering improve reliability.</p> <p>Next: We can explore regularization (Ridge/Lasso) or interaction terms &amp; feature engineering.</p>"},{"location":"notes/note11/","title":"Feature Reduction Using Principal Component Analysis (PCA)","text":"<p>Principal Component Analysis (PCA) is a linear, unsupervised technique that projects data into a lower-dimensional space while retaining as much variance (information) as possible. It creates new orthogonal axes (principal components) ordered by the amount of variance they capture.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#1-why-feature-reduction","title":"1. Why Feature Reduction?","text":"<p>High-dimensional data can cause: - Redundant / correlated features (multicollinearity) - Increased model variance (overfitting risk) - Higher computational costs - Harder visualization &amp; interpretability</p> <p>PCA addresses these by creating a smaller set of uncorrelated components still carrying most of the signal.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#2-core-intuition","title":"2. Core Intuition","text":"<p>Imagine fitting a line through a 2D point cloud so that when you project points onto that line, the spread (variance) is maximized. That line is the first principal component. The second component is the next axis (orthogonal to the first) capturing the next largest remaining variance, and so on.</p> <p>PCA is fundamentally about variance maximization under orthogonality constraints.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#3-mathematical-foundations","title":"3. Mathematical Foundations","text":"<p>Given a centered data matrix \\(X \\in \\mathbb{R}^{n \\times d}\\) (n samples, d features):</p> <ol> <li>Center features: subtract mean of each column: \\(X_c = X - \\mu\\).</li> <li>(Optional) Scale (standardize) features if they are on different units.</li> <li>Compute covariance matrix: \\(\\Sigma = \\frac{1}{n-1} X_c^T X_c\\).</li> <li>Eigen-decompose: \\(\\Sigma = Q \\Lambda Q^T\\) where:</li> <li>Columns of \\(Q\\) are eigenvectors (principal directions)</li> <li>Diagonal of \\(\\Lambda\\) has eigenvalues \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0\\)</li> <li>Variance explained by component \\(k\\): \\(\\text{VE}_k = \\frac{\\lambda_k}{\\sum_{j=1}^d \\lambda_j}\\)</li> <li>Project data onto first \\(K\\) components: \\(Z = X_c Q_K\\) (where \\(Q_K\\) are the first \\(K\\) eigenvectors)</li> </ol>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#relation-to-singular-value-decomposition-svd","title":"Relation to Singular Value Decomposition (SVD)","text":"<p>Instead of forming \\(\\Sigma\\), compute SVD: \\(X_c = U S V^T\\). - Right singular vectors (columns of \\(V\\)) are eigenvectors of \\(\\Sigma\\). - Eigenvalues relate to singular values: \\(\\lambda_k = \\frac{S_k^2}{n-1}\\). SVD is numerically more stable for large or sparse matrices.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#orthogonality-uncorrelated-components","title":"Orthogonality &amp; Uncorrelated Components","text":"<p>Because \\(Q^T Q = I\\), the transformed features (principal components) are uncorrelated. This helps regression models sensitive to multicollinearity (e.g., linear regression) but at the cost of losing direct feature semantics.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#4-step-by-step-pca-workflow","title":"4. Step-by-Step PCA Workflow","text":"<ol> <li>Collect feature matrix \\(X\\) (shape: n samples \u00d7 d features)</li> <li>Optional: Handle missing values / outliers (extreme outliers inflate variance)</li> <li>Decide whether to standardize (usually yes if units differ)</li> <li>Center (and scale) the data</li> <li>Compute covariance (or use SVD directly)</li> <li>Sort eigenvalues/eigenvectors</li> <li>Compute cumulative explained variance</li> <li>Select number of components K (threshold, elbow, downstream performance)</li> <li>Project data: \\(Z = X_c Q_K\\)</li> <li>Use \\(Z\\) in modeling / visualization</li> </ol>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#5-choosing-the-number-of-components","title":"5. Choosing the Number of Components","text":"<p>Common heuristics: - Cumulative variance threshold (e.g., retain 90\u201395%) - Scree plot elbow (point where marginal gain drops) - Cross-validation of downstream model performance - Information criteria (rarely in practice for PCA directly)</p> <p>No universal rule\u2014depends on performance vs. interpretability trade-offs.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#6-practical-considerations","title":"6. Practical Considerations","text":"Issue Guidance Scaling Always scale if units differ; PCA is variance-sensitive. Outliers Can dominate components; consider robust scaling / winsorization. Interpretability Components are linear mixtures; inspect loadings. Sparsity PCA does not produce sparse features; consider Sparse PCA if needed. Non-linearity PCA only captures linear correlations; use Kernel PCA / t-SNE / UMAP for non-linear structure. Leakage Fit PCA only on training set; apply same transform to test set.","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#7-interpreting-component-loadings","title":"7. Interpreting Component Loadings","text":"<p>Loadings = eigenvectors (or columns of the rotation matrix). Large absolute value =&gt; strong contribution of original feature to that component.</p> <p>For standardized data, the squared loading approximates the proportion of variance of a feature captured by the component.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#8-python-example-iris-dataset","title":"8. Python Example (Iris Dataset)","text":"<p>Below: Standardize, fit PCA, inspect explained variance, plot scree &amp; 2D projection.</p> Python<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. Load data\niris = load_iris()\nX = iris.data  # shape (150, 4)\ny = iris.target\nfeature_names = iris.feature_names\n\n# 2. Standardize (mean=0, var=1)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Fit PCA (all components first)\npca = PCA(n_components=None, random_state=42)\nX_pca = pca.fit_transform(X_scaled)\n\nexplained = pca.explained_variance_ratio_\ncumulative = np.cumsum(explained)\n\nprint(\"Explained variance ratio per component:\")\nfor i, (ev, cv) in enumerate(zip(explained, cumulative), start=1):\n    print(f\"PC{i}: {ev:.4f} (cumulative: {cv:.4f})\")\n\n# 4. Scree plot\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(explained)+1), explained, marker='o', label='Individual')\nplt.plot(range(1, len(explained)+1), cumulative, marker='s', label='Cumulative')\nplt.xticks(range(1, len(explained)+1))\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.title('PCA Scree Plot (Iris)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 5. 2D projection (first two components)\npc_df = pd.DataFrame(X_pca[:, :2], columns=['PC1', 'PC2'])\npc_df['target'] = y\nplt.figure(figsize=(6,5))\nsns.scatterplot(data=pc_df, x='PC1', y='PC2', hue='target', palette='viridis', s=60)\nplt.title('Iris Projected onto First Two Principal Components')\nplt.tight_layout()\nplt.show()\n\n# 6. Component loadings (contribution of original features)\nloadings = pd.DataFrame(pca.components_.T, index=feature_names,\n                        columns=[f'PC{i}' for i in range(1, len(feature_names)+1)])\nprint(\"\\nComponent Loadings (first two PCs):\")\nprint(loadings.iloc[:, :2])\n</code></pre>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#9-when-pca-helps","title":"9. When PCA Helps","text":"<ul> <li>Visualization: 2D/3D scatter of high-dimensional data</li> <li>Noise reduction (drop low-variance components)</li> <li>Mitigating multicollinearity before linear models</li> <li>Preprocessing before clustering or regression</li> <li>Speedup for algorithms sensitive to high dimensionality</li> </ul>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#10-when-pca-may-not-help","title":"10. When PCA May Not Help","text":"<ul> <li>Strong non-linear manifolds (consider Kernel PCA, t-SNE, UMAP)</li> <li>Features already low-dimensional and well-engineered</li> <li>Interpretability is critical (components obscure original meanings)</li> <li>Presence of many categorical features (PCA requires numeric, usually continuous)</li> </ul>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#11-limitations-assumptions","title":"11. Limitations &amp; Assumptions","text":"<ul> <li>Linear method: captures only linear covariance</li> <li>Maximizes variance, not class separation (unsupervised)</li> <li>Sensitive to scaling and outliers</li> <li>Components can be hard to interpret in regulated or explainability-critical domains</li> </ul>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#12-summary","title":"12. Summary","text":"<p>PCA re-expresses data through orthogonal axes sorted by variance. Properly applied (with scaling and careful component selection), it improves efficiency and can enhance downstream performance. Always validate whether the reduction preserves task-relevant information.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#13-next-topics-suggestions","title":"13. Next Topics (Suggestions)","text":"<ul> <li>Linear Discriminant Analysis (LDA) \u2013 supervised dimensionality reduction</li> <li>Kernel PCA \u2013 non-linear extension</li> <li>t-SNE / UMAP \u2013 manifold visualization</li> <li>Feature Selection vs Feature Extraction \u2013 conceptual distinctions</li> <li>Sparse PCA \u2013 interpretability via sparsity constraints</li> </ul>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note11/#quick-reference","title":"Quick Reference","text":"Concept Formula / Idea Covariance Matrix \\(\\Sigma = \\frac{1}{n-1} X_c^T X_c\\) Eigen Decomposition \\(\\Sigma = Q \\Lambda Q^T\\) Variance Explained \\(\\text{VE}_k = \\lambda_k / \\sum_j \\lambda_j\\) Projection \\(Z = X_c Q_K\\) SVD Link \\(X_c = U S V^T\\), \\(\\lambda_k = S_k^2/(n-1)\\) <p>Tip: Always fit PCA on the training split, then transform validation/test sets with the fitted object to avoid data leakage.</p>","tags":["pca","dimensionality-reduction","feature-engineering","unsupervised-learning"]},{"location":"notes/note12/","title":"Implementation of Regression Models on Various Datasets","text":"<p>This note walks through hands-on implementations of common regression (and a related classification) techniques using small synthetic and real datasets. You will see how to construct models, evaluate them, and understand where each approach fits.</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#1-simple-linear-regression-salary-vs-experience","title":"1. Simple Linear Regression (Salary vs. Experience)","text":"<p>Goal: Predict salary from years of experience.</p> <p>Concept: Fit a line \\( y = \\beta_0 + \\beta_1 x + \\varepsilon \\) minimizing squared error.</p> Python<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# Sample dataset\ndata = {\n    \"YearsExperience\": [1,2,3,4,5,6,7,8,9,10],\n    \"Salary\": [40000,45000,50000,60000,65000,70000,75000,80000,85000,90000]\n}\ndf = pd.DataFrame(data)\n\nX = df[[\"YearsExperience\"]]  # feature matrix (2D)\ny = df[\"Salary\"]\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = model.predict(X)\n\nprint(\"Intercept:\", model.intercept_)\nprint(\"Slope:\", model.coef_[0])\nprint(\"R\u00b2:\", r2_score(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\n\nplt.scatter(X, y, color=\"steelblue\", label=\"Actual\")\nplt.plot(X, y_pred, color=\"crimson\", label=\"Predicted\")\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.title(\"Simple Linear Regression\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Note: This synthetic dataset is nearly linear; real-world salary data usually requires more robust modeling and additional predictors.</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#2-multiple-linear-regression-boston-housing-deprecated-example","title":"2. Multiple Linear Regression (Boston Housing \u2013 Deprecated Example)","text":"<p>The classic Boston Housing dataset (median house value vs. socioeconomic &amp; structural features) is deprecated in scikit-learn due to ethical concerns and should not be used in production or teaching without context. We'll illustrate the API, then recommend an alternative.</p> Python<pre><code>import pandas as pd\nfrom sklearn.datasets import load_boston  # Deprecated in recent versions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Warning: load_boston may be removed; wrap in try/except\ntry:\n    boston = load_boston()\n    X = pd.DataFrame(boston.data, columns=boston.feature_names)\n    y = boston.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    print(\"Test MSE:\", mean_squared_error(y_test, y_pred))\n    print(\"Test R\u00b2:\", r2_score(y_test, y_pred))\nexcept Exception as e:\n    print(\"Boston dataset not available (expected in newer sklearn). Use California Housing instead.\")\n    print(\"Error:\", e)\n</code></pre>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#recommended-replacement-california-housing-see-section-5","title":"Recommended Replacement: California Housing (see section 5)","text":"<p>Key points in multiple regression: - Check multicollinearity (VIF) - Scale features if magnitudes differ significantly - Consider regularization (Ridge/Lasso) when many correlated predictors</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#3-polynomial-regression-modeling-non-linear-relationships","title":"3. Polynomial Regression (Modeling Non-linear Relationships)","text":"<p>Polynomial regression augments features with polynomial terms then applies linear regression in the expanded feature space.</p> Python<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = np.array([1,2,3,4,5,6]).reshape(-1, 1)\ny = np.array([2, 6, 14, 28, 45, 66])  # Quadratic-like growth\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n\nprint(\"R\u00b2 (degree=2):\", r2_score(y, y_pred))\nprint(\"Feature names:\", poly.get_feature_names_out([\"X\"]))\n\nplt.scatter(X, y, color=\"royalblue\", label=\"Actual\")\nplt.plot(X, y_pred, color=\"darkorange\", label=\"Polynomial Fit (deg 2)\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Polynomial Regression\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Caution: High-degree polynomials can overfit; use cross-validation and consider regularization or splines.</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#using-a-pipeline-good-practice","title":"Using a Pipeline (Good Practice)","text":"Python<pre><code>from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    (\"poly\", PolynomialFeatures(degree=3, include_bias=False)),\n    (\"linreg\", LinearRegression())\n])\npipeline.fit(X, y)\nprint(\"R\u00b2 (degree=3):\", pipeline.score(X, y))\n</code></pre>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#4-logistic-regression-classification-contrast-iris-dataset","title":"4. Logistic Regression (Classification Contrast \u2013 Iris Dataset)","text":"<p>Although named \"regression\", Logistic Regression is a classification algorithm modeling log-odds.</p> Python<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\niris = load_iris()\nX = iris.data[:, :2]  # use two features for visualization simplicity\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\nclf = LogisticRegression(max_iter=300, multi_class='auto')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n</code></pre> <p>For multi-class probability calibration or imbalanced classes, consider <code>solver='lbfgs'</code> (default), penalty adjustments, or class weights.</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#5-real-world-regression-california-housing-dataset","title":"5. Real-World Regression: California Housing Dataset","text":"<p>Predict median house value in California districts. Each row aggregates census block-level stats (continuous target: <code>MedHouseValue</code>).</p> Python<pre><code>from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport pandas as pd\n\nhousing = fetch_california_housing()\nX = pd.DataFrame(housing.data, columns=housing.feature_names)\ny = housing.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nprint(\"Test MSE:\", mean_squared_error(y_test, preds))\nprint(\"Test RMSE:\", mean_squared_error(y_test, preds, squared=False))\nprint(\"Test R\u00b2:\", r2_score(y_test, preds))\n</code></pre>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#potential-enhancements","title":"Potential Enhancements","text":"<ul> <li>Add feature scaling + regularization (Ridge/Lasso)</li> <li>Log-transform skewed features (e.g., population)</li> <li>Evaluate residual diagnostics (see earlier residuals note)</li> <li>Compare tree-based models (RandomForest, GradientBoosting)</li> </ul>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#6-summary","title":"6. Summary","text":"Scenario Technique Notes Single numeric predictor Simple Linear Regression Interpret slope directly Multiple correlated predictors Multiple Linear Regression Watch multicollinearity &amp; consider regularization Curved relationship Polynomial Regression Control degree to avoid overfit Categorical target (multi-class) Logistic Regression Outputs class probabilities/log-odds Real estate aggregated census data California Housing Regression Baseline; test advanced models","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Always separate training and test sets for realistic performance estimates.</li> <li>Use pipelines for transformations + models to avoid leakage.</li> <li>Prefer modern, ethically appropriate datasets (California over Boston).</li> <li>Polynomial features increase dimensionality quickly\u2014apply regularization and validation.</li> <li>Logistic Regression belongs in classification despite its name\u2014don\u2019t use for continuous targets.</li> </ul>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note12/#next-directions","title":"Next Directions","text":"<ul> <li>Add Ridge/Lasso/ElasticNet comparisons</li> <li>Introduce regularization paths &amp; bias-variance trade-off</li> <li>Move to tree ensembles &amp; boosting for non-linearities</li> <li>Implement cross-validation &amp; hyperparameter tuning (GridSearchCV / RandomizedSearchCV)</li> </ul> <p>Tip: Wrap repeated evaluation logic in utility functions (e.g., <code>evaluate_regression(model, X_train, X_test, y_train, y_test)</code>) to standardize metrics across experiments.</p>","tags":["regression","linear-regression","polynomial-regression","logistic-regression","implementation"]},{"location":"notes/note13/","title":"Uncertainty &amp; Probabilistic Models","text":"<p>Modeling distributions.</p>","tags":["probabilistic","bayesian"]},{"location":"notes/note13/#concepts","title":"Concepts","text":"<ul> <li>Bayesian Inference</li> <li>Likelihood</li> <li>Prior / Posterior</li> </ul>","tags":["probabilistic","bayesian"]},{"location":"notes/note14/","title":"Ethics &amp; Fairness","text":"<p>Responsible AI principles.</p>","tags":["ethics","fairness"]},{"location":"notes/note14/#topics","title":"Topics","text":"<ul> <li>Bias</li> <li>Transparency</li> <li>Accountability</li> </ul>","tags":["ethics","fairness"]},{"location":"notes/note15/","title":"Deployment &amp; Monitoring","text":"<p>Putting models in production.</p>","tags":["deployment","monitoring"]},{"location":"notes/note15/#steps","title":"Steps","text":"<ul> <li>Packaging</li> <li>Serving</li> <li>Observability</li> </ul>","tags":["deployment","monitoring"]},{"location":"notes/note2/","title":"Supervised vs Unsupervised Learning (Based on Problem Definition)","text":""},{"location":"notes/note2/#1-supervised-learning","title":"1. Supervised Learning","text":"<p>Problem Definition - The problem is defined in terms of input variables (features) and output variables (labels/targets). - The task is to learn a mapping function from inputs \u2192 outputs. - The goal is to predict outcomes for unseen data.  </p> <p>Examples of Problems - Classification \u2192 Predict categories (e.g., spam/not spam, disease/no disease). - Regression \u2192 Predict continuous values (e.g., house price, stock value).  </p> <p>Key Point</p> <p>In supervised learning, we know the \"correct answers\" during training.  </p>"},{"location":"notes/note2/#2-unsupervised-learning","title":"2. Unsupervised Learning","text":"<p>Problem Definition - The problem is defined only in terms of input variables (features). - No output/labels are given. - The task is to discover hidden patterns, structures, or relationships in the data.  </p> <p>Examples of Problems - Clustering \u2192 Grouping similar items (e.g., customer segmentation, document clustering). - Dimensionality Reduction \u2192 Reducing features while preserving patterns (e.g., PCA). - Association Rule Mining \u2192 Finding relations (e.g., \"people who buy bread often buy butter\").  </p> <p>Key Point</p> <p>In unsupervised learning, the system learns without knowing the correct answers.  </p>"},{"location":"notes/note2/#comparison-table-based-on-problem-definition","title":"\u2705 Comparison Table (Based on Problem Definition)","text":"Aspect Supervised Learning Unsupervised Learning Problem Setup Input + Output (labeled data) Only Input (unlabeled data) Goal Learn mapping from input \u2192 output Discover hidden patterns/structure Training Data Requires labeled data Works with unlabeled data Problem Type Prediction (classification, regression) Exploration (clustering, association, dimensionality reduction) Examples Predict house prices, spam detection, disease diagnosis Customer segmentation, market basket analysis, PCA"},{"location":"notes/note3/","title":"Understanding the Problem and Its Possible Solutions Using IRIS and Various Datasets","text":""},{"location":"notes/note3/#step-1-understanding-the-problem","title":"Step 1: Understanding the Problem","text":"<p>When working with a dataset (like IRIS, Titanic, MNIST, etc.), the first step is to define:</p> <ul> <li>What is given? \u2192 Input features  </li> <li>What do we want to find? \u2192 Output / Patterns  </li> <li>Is the output labeled? </li> </ul> <p>Learning Type</p> <ul> <li>If YES \u2192 Supervised Learning  </li> <li>If NO \u2192 Unsupervised Learning  </li> </ul>"},{"location":"notes/note3/#step-2-iris-dataset-example","title":"Step 2: IRIS Dataset Example","text":"<p>The IRIS dataset has:</p> <ul> <li>Input (features): Sepal length, Sepal width, Petal length, Petal width  </li> <li>Output (label): Species (Setosa, Versicolor, Virginica)  </li> </ul>"},{"location":"notes/note3/#possible-problems-solutions","title":"Possible Problems &amp; Solutions","text":"<ul> <li>Classification (Supervised) </li> <li>Problem: Predict flower species from features  </li> <li> <p>Solution: Logistic Regression, SVM, Decision Tree, KNN  </p> </li> <li> <p>Clustering (Unsupervised) </p> </li> <li>Problem: Group flowers without knowing species labels  </li> <li> <p>Solution: K-Means, Hierarchical Clustering  </p> </li> <li> <p>Dimensionality Reduction (Unsupervised) </p> </li> <li>Problem: Visualize high-dimensional flower data in 2D  </li> <li>Solution: PCA (Principal Component Analysis)  </li> </ul>"},{"location":"notes/note3/#step-3-other-dataset-examples","title":"Step 3: Other Dataset Examples","text":""},{"location":"notes/note3/#1-titanic-dataset-passenger-survival","title":"1. Titanic Dataset (Passenger Survival)","text":"<ul> <li>Features: Age, Sex, Ticket class, Fare, etc.  </li> <li>Label: Survival (Yes/No)  </li> <li>Problem: Predict survival  </li> <li>Solution: Classification \u2192 Logistic Regression, Decision Tree  </li> </ul>"},{"location":"notes/note3/#2-mnist-dataset-handwritten-digits","title":"2. MNIST Dataset (Handwritten Digits)","text":"<ul> <li>Features: Pixel values of images (28\u00d728)  </li> <li>Label: Digit (0\u20139)  </li> <li>Problem: Digit recognition  </li> <li>Solution: Classification (Supervised, Deep Learning CNNs)  </li> </ul>"},{"location":"notes/note3/#3-supermarket-sales-dataset","title":"3. Supermarket Sales Dataset","text":"<ul> <li>Features: Product, Price, Quantity, Date, etc.  </li> <li>Label: Not always given  </li> </ul> <p>Problems &amp; Solutions: - Predict sales revenue \u2192 Regression (Supervised) - Customer segmentation \u2192 Clustering (Unsupervised) - Market basket analysis \u2192 Association Rule Mining (Unsupervised)  </p>"},{"location":"notes/note3/#4-covid-19-dataset","title":"4. COVID-19 Dataset","text":"<ul> <li>Features: Cases, Deaths, Vaccinations, Date, Country  </li> </ul> <p>Problems &amp; Solutions: - Forecast future cases \u2192 Time Series Prediction (Supervised) - Cluster countries by impact \u2192 Clustering (Unsupervised) - Identify main factors influencing spread \u2192 Feature Selection + Regression  </p>"},{"location":"notes/note3/#step-4-general-rule-for-problem-solution","title":"Step 4: General Rule for Problem \u2192 Solution","text":"<ul> <li>If labels (target variable) are present \u2192 Supervised Learning (Classification/Regression)  </li> <li>If no labels are present \u2192 Unsupervised Learning (Clustering/Association)  </li> <li>If the task is decision-making via rewards \u2192 Reinforcement Learning  </li> </ul>"},{"location":"notes/note3/#in-summary","title":"\u2705 In Summary","text":"<ul> <li>IRIS \u2192 Classification (Supervised), Clustering (Unsupervised), PCA (Dimensionality Reduction)  </li> <li>Titanic \u2192 Classification  </li> <li>MNIST \u2192 Classification (Deep Learning)  </li> <li>Supermarket Sales \u2192 Regression, Clustering, Association Rules  </li> <li>COVID-19 \u2192 Time Series Forecasting, Clustering  </li> </ul>"},{"location":"notes/note4/","title":"Machine Learning Notes with Python Examples","text":""},{"location":"notes/note4/#1-supervised-vs-unsupervised-learning","title":"1. Supervised vs Unsupervised Learning","text":""},{"location":"notes/note4/#supervised-learning-example-classification","title":"Supervised Learning Example (Classification)","text":"Python<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load IRIS dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train model\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Predict &amp; evaluate\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"notes/note4/#python-libraries-suitable-for-machine-learning","title":"Python Libraries Suitable for Machine Learning","text":"<p>In Python, Machine Learning (ML) is supported by a powerful ecosystem of libraries spanning data handling, model building, visualization, experimentation, and deployment.</p> <p>Below is a categorized, skimmable overview you can reference while choosing tools for a project.</p>"},{"location":"notes/note4/#1-core-libraries-for-data-handling","title":"\ud83d\udd39 1. Core Libraries for Data Handling","text":"Library Purpose Notes NumPy Numerical computing (n\u2011d arrays, linear algebra, vectorization) Foundation for most libraries Pandas Data manipulation (DataFrames, IO: CSV/Parquet/Excel) Feature engineering &amp; preprocessing SciPy Scientific computing (stats, optimization, signal processing) Builds on NumPy; specialized routines"},{"location":"notes/note4/#2-machine-learning-libraries-classical-gradient-boosting","title":"\ud83d\udd39 2. Machine Learning Libraries (Classical &amp; Gradient Boosting)","text":"Library Focus Strength Scikit-learn Classification, regression, clustering, preprocessing, model selection Standard, consistent API XGBoost Gradient boosting (trees) Strong for tabular; regularization controls LightGBM Fast gradient boosting Large datasets, leaf-wise growth CatBoost Gradient boosting with native categorical support Minimal encoding needed"},{"location":"notes/note4/#3-deep-learning-libraries","title":"\ud83d\udd39 3. Deep Learning Libraries","text":"Library Focus Notes TensorFlow Scalable deep learning &amp; deployment Ecosystem: TF Serving, TF Lite Keras High-level API (now core TensorFlow) Fast prototyping PyTorch Dynamic computation graphs; research-friendly Strong community &amp; ecosystem (TorchVision, TorchText) MXNet Scalable distributed DL Multi-language bindings <p>Tip: For research \u2192 PyTorch; for production edge / mobile \u2192 TensorFlow (TF Lite); for quick educational prototypes \u2192 Keras.</p>"},{"location":"notes/note4/#4-visualization-model-insight","title":"\ud83d\udd39 4. Visualization &amp; Model Insight","text":"Library Focus When to Use Matplotlib Base plotting primitives Low-level control Seaborn Statistical &amp; aesthetic wrappers over Matplotlib Fast EDA Plotly Interactive charts &amp; dashboards Exploratory analytics / web Yellowbrick Model diagnostic visualizers Confusion matrix, ROC, residuals"},{"location":"notes/note4/#5-natural-language-processing-nlp","title":"\ud83d\udd39 5. Natural Language Processing (NLP)","text":"Library Focus Notes NLTK Classical NLP (tokenization, stemming, POS) Education &amp; traditional pipelines spaCy Industrial NLP (NER, parsing, vectors) Efficient, production-friendly Transformers (Hugging Face) Pre-trained large models (BERT, GPT, T5) Transfer learning powerhouse Gensim Topic modeling &amp; embeddings (Word2Vec, Doc2Vec, LSI) Lightweight semantic modeling"},{"location":"notes/note4/#6-computer-vision-cv","title":"\ud83d\udd39 6. Computer Vision (CV)","text":"Library Focus Notes OpenCV Image processing &amp; classical CV Pre/post-processing, feature ops scikit-image Image filtering, transforms, feature extraction Complement to OpenCV Detectron2 Object detection &amp; segmentation PyTorch; state-of-the-art reference MMCV / MMDetection OpenMMLab modular vision frameworks Flexible experiment structure"},{"location":"notes/note4/#7-reinforcement-learning","title":"\ud83d\udd39 7. Reinforcement Learning","text":"Library Focus Notes Gym (OpenAI Gym / Gymnasium) Standardized RL environments Benchmarking tasks Stable-Baselines3 Ready-to-use RL algorithms (PPO, DQN, A2C, etc.) Quick baseline training RLlib Scalable RL (Ray ecosystem) Distributed training"},{"location":"notes/note4/#8-model-deployment-utilities","title":"\ud83d\udd39 8. Model Deployment &amp; Utilities","text":"Library / Tool Focus Notes ONNX Model exchange format Interoperability across runtimes MLflow Experiment tracking, model registry, deployment Reproducibility &amp; lifecycle TensorFlow Serving Serving TensorFlow models High-performance inference FastAPI / Flask Serving models via REST APIs Lightweight microservices <p>Deployment pathways often combine: Train (PyTorch) \u2192 Export (ONNX) \u2192 Serve (ONNX Runtime / FastAPI) or Train (TensorFlow) \u2192 Serve (TF Serving / Vertex / SageMaker).</p>"},{"location":"notes/note4/#summary-cheat-sheet","title":"\u2705 Summary Cheat Sheet","text":"Category Primary Picks Data Handling NumPy, Pandas, SciPy ML (Classical + Boosting) Scikit-learn, XGBoost, LightGBM, CatBoost Deep Learning PyTorch, TensorFlow, Keras Visualization Matplotlib, Seaborn, Plotly, Yellowbrick NLP spaCy, Transformers, NLTK, Gensim Computer Vision OpenCV, scikit-image, Detectron2 RL Gym, Stable-Baselines3, RLlib Deployment MLflow, FastAPI, ONNX"},{"location":"notes/note4/#selection-guidance","title":"Selection Guidance","text":"Need Likely Choice Tabular baseline Scikit-learn Tabular SOTA model LightGBM / CatBoost Prototype deep model quickly Keras Research flexibility PyTorch Production pipeline w/ tracking MLflow + FastAPI Classic statistical transforms SciPy Explain model predictions Yellowbrick (viz) / SHAP (not listed above) Transfer learning NLP Transformers <p>Next Expansion Ideas: Add time-series specific libs (Prophet, statsmodels, darts), MLOps orchestration (Prefect, Airflow, Dagster), and model explainability (SHAP, LIME, Captum) in a future revision.</p>"},{"location":"notes/note5/","title":"Environment Setup &amp; Installation of Important ML Libraries","text":"<p>This guide walks through installing Python, creating an isolated environment, and installing essential libraries for machine learning, deep learning, NLP, computer vision, reinforcement learning, and deployment.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#1-install-python","title":"1. Install Python","text":"<p>Download from: https://www.python.org (use version 3.8+; 3.10\u20133.12 commonly supported). Or install Anaconda / Miniconda (recommended for ML\u2014many packages pre-built, easier dependency handling).</p> <p>Check version: Bash<pre><code>python --version\n</code></pre></p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#2-set-up-a-virtual-environment","title":"2. Set Up a Virtual Environment","text":"<p>Creating an isolated environment prevents dependency conflicts across projects.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#using-venv-built-in","title":"Using <code>venv</code> (built-in)","text":"Bash<pre><code>python -m venv ml_env\n# Activate (Windows PowerShell)\nml_env\\Scripts\\Activate.ps1\n# Or Windows cmd\nml_env\\Scripts\\activate.bat\n# Or macOS / Linux\nsource ml_env/bin/activate\n</code></pre>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#using-conda","title":"Using <code>conda</code>","text":"Bash<pre><code>conda create --name ml_env python=3.9\nconda activate ml_env\n</code></pre> <p>Choose conda if you need complex native dependencies (e.g., some CV or GPU builds) without manual compilation.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#3-install-essential-libraries","title":"3. Install Essential Libraries","text":"<p>Install in logical groups so failures are easier to diagnose.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#core-libraries","title":"\u2705 Core Libraries","text":"Bash<pre><code>pip install numpy pandas scipy\n</code></pre>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#machine-learning-classical-boosting","title":"\u2705 Machine Learning (Classical &amp; Boosting)","text":"Bash<pre><code>pip install scikit-learn xgboost lightgbm catboost\n</code></pre>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#deep-learning","title":"\u2705 Deep Learning","text":"Bash<pre><code>pip install tensorflow keras torch torchvision\n</code></pre> <p>If you have a CUDA-capable GPU, consult PyTorch / TensorFlow install pages for GPU-specific wheels.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#data-visualization","title":"\u2705 Data Visualization","text":"Bash<pre><code>pip install matplotlib seaborn plotly yellowbrick\n</code></pre>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#natural-language-processing","title":"\u2705 Natural Language Processing","text":"<p>Bash<pre><code>pip install nltk spacy transformers gensim\n</code></pre> Download a spaCy model: Bash<pre><code>python -m spacy download en_core_web_sm\n</code></pre></p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#computer-vision","title":"\u2705 Computer Vision","text":"Bash<pre><code>pip install opencv-python scikit-image\n</code></pre>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#reinforcement-learning","title":"\u2705 Reinforcement Learning","text":"Bash<pre><code>pip install gym stable-baselines3\n</code></pre> <p>For Atari or MuJoCo environments additional extras may be required (e.g., <code>pip install gym[atari]</code>).</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#deployment-utilities","title":"\u2705 Deployment &amp; Utilities","text":"<p>Bash<pre><code>pip install flask fastapi uvicorn mlflow onnx\n</code></pre> (Optional) ONNX runtime for inference: Bash<pre><code>pip install onnxruntime\n</code></pre></p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#4-jupyter-notebook-ide-setup","title":"4. Jupyter Notebook / IDE Setup","text":"<p>Install Jupyter Lab / Notebook: Bash<pre><code>pip install jupyterlab notebook\n</code></pre> Run Notebook: Bash<pre><code>jupyter notebook\n</code></pre></p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#ide-options","title":"IDE Options","text":"<ul> <li>VS Code (Python + Jupyter extensions)</li> <li>PyCharm (robust project tooling)</li> <li>Google Colab (no local install, free GPU tier)</li> <li>Kaggle Notebooks (datasets + accelerators)</li> </ul>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#5-verify-installation","title":"5. Verify Installation","text":"<p>Open Python and try imports: Python<pre><code>import numpy as np\nimport pandas as pd\nimport sklearn\nimport tensorflow as tf\nimport torch\nimport matplotlib.pyplot as plt\n\nprint(\"All libraries loaded successfully!\")\n</code></pre> If no errors appear, your environment is ready.</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#summary","title":"\u2705 Summary","text":"Step Action 1 Install Python / Anaconda 2 Create virtual or conda environment 3 Install core + ML + DL + NLP + CV + RL + deployment libraries 4 Set up Jupyter / IDE 5 Test imports","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note5/#optional-next-steps","title":"Optional Next Steps","text":"Goal Command / Tool Freeze dependencies <code>pip freeze &gt; requirements.txt</code> Upgrade a package <code>pip install -U package_name</code> Check outdated <code>pip list --outdated</code> Manage experiments <code>mlflow ui</code> Serve FastAPI app <code>uvicorn app:app --reload</code> <p>Keep environments lean. Only install what the specific project requires; create new environments for materially different dependency stacks (e.g., GPU deep learning vs lightweight analytics).</p>","tags":["setup","environment","python","installation","tooling"]},{"location":"notes/note6/","title":"What is Linear Regression?","text":"<p>Linear Regression is a supervised learning algorithm used to predict a continuous outcome. It models the relationship between one or more independent variables (features) and a dependent variable (target) by fitting a line (in higher dimensions: a hyperplane) that minimizes error.</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#core-idea-formula","title":"Core Idea / Formula","text":"<p>For a simple (single feature) linear regression the model is:</p> \\[Y = mX + c + \\varepsilon\\] <p>Where: - \\(Y\\) = Dependent variable (target) - \\(X\\) = Independent variable (feature) - \\(m\\) = Slope (coefficient / weight) - \\(c\\) = Intercept (bias term) - \\(\\varepsilon\\) = Error term (residual noise)</p> <p>For multiple features (\\(n\\) features):</p> \\[Y = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_n X_n + \\varepsilon\\] <p>Vector form: \\(\\hat{y} = Xw\\) (with intercept handled via bias or augmented feature vector).</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#types-of-linear-regression","title":"Types of Linear Regression","text":"<ul> <li>Simple Linear Regression \u2013 One independent variable (\\(X\\)).   Example: Predicting house price using only square footage.</li> <li>Multiple Linear Regression \u2013 Multiple independent variables (\\(X_1, X_2, \\dots, X_n\\)).   Example: Predicting house price using size, rooms, location, age.</li> </ul> <p>Variants (later study): Ridge, Lasso, Elastic Net (regularized linear models).</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#steps-in-linear-regression","title":"Steps in Linear Regression","text":"<ol> <li>Collect dataset.</li> <li>Define dependent variable (\\(Y\\)) and independent features (\\(X\\)).</li> <li>Split data (train/test) if doing generalization evaluation.</li> <li>Fit model using Ordinary Least Squares (OLS) \u2192 minimizes Sum of Squared Errors.</li> <li>Generate predictions on training (and test) data.</li> <li>Evaluate performance (MSE, RMSE, \\(R^2\\)).</li> <li>Inspect residuals for patterns (check assumptions).</li> </ol>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#python-example-simple-linear-regression","title":"Python Example (Simple Linear Regression)","text":"Python<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Sample dataset: Study Hours vs Scores\ndata = {\n    'Hours': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Scores': [10, 20, 30, 40, 50, 55, 65, 80, 90]\n}\n\ndf = pd.DataFrame(data)\n\n# Feature matrix (must be 2D) and target vector\nX = df[['Hours']]  # shape (n, 1)\ny = df['Scores']\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predictions\ny_pred = model.predict(X)\n\n# Coefficients\nprint(\"Slope (m):\", model.coef_[0])\nprint(\"Intercept (c):\", model.intercept_)\n\n# Evaluation\nprint(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\nprint(\"R^2 Score:\", r2_score(y, y_pred))\n\n# Visualization\nplt.scatter(X, y, color='blue', label='Actual')\nplt.plot(X, y_pred, color='red', label='Fitted Line')\nplt.xlabel(\"Study Hours\")\nplt.ylabel(\"Scores\")\nplt.title(\"Linear Regression Example\")\nplt.legend()\nplt.show()\n</code></pre>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#key-evaluation-metrics","title":"Key Evaluation Metrics","text":"Metric Definition Notes MSE (Mean Squared Error) \\(\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\\) Penalizes larger errors strongly RMSE (Root MSE) \\(\\sqrt{\\text{MSE}}\\) Same units as target \\(R^2\\) Score \\(1 - \\frac{\\text{SS}_{res}}{\\text{SS}_{tot}}\\) Proportion of variance explained","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#residual-diagnostics","title":"Residual Diagnostics","text":"<p>Understanding residuals (errors) is critical to validate linear regression assumptions.</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#what-are-residuals","title":"What Are Residuals?","text":"<p>Residual \\(e_i = y_i - \\hat{y}_i\\) (difference between actual and predicted). A good linear model leaves random, structureless residuals.</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#key-assumptions-checked-via-residuals","title":"Key Assumptions Checked via Residuals","text":"Assumption What to Look For Violation Pattern Linearity No systematic curve in residuals vs fitted U-shape or curvature Homoscedasticity (constant variance) Even vertical spread Funnel (widening/narrowing) Independence No obvious time/order pattern Trend / cycles Normality (for inference) Residuals approx. normal Heavy tails / skew in histogram &amp; Q-Q No high leverage / influence Points not dominating fit Outliers far from trend","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#common-diagnostic-plots","title":"Common Diagnostic Plots","text":"Plot Purpose Healthy Pattern Residuals vs Fitted Detect non-linearity &amp; heteroscedasticity Horizontal cloud around 0 Histogram / KDE of Residuals Assess normality Bell-shaped, centered at 0 Q-Q Plot Quantify deviation from normal Points ~ straight line Residuals vs Feature (each X) Check overlooked non-linearity No structure Scale-Location ( resid ^0.5 vs fitted) Leverage vs Residual (Cook\u2019s Distance) Influential points Few, small influence","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#remedies-when-issues-found","title":"Remedies When Issues Found","text":"Issue Potential Fixes Non-linearity Add polynomial terms, interactions, transform features Heteroscedasticity Transform target (log), weighted least squares, robust regression Non-normal residuals Often harmless with large n; else transform or use robust methods Autocorrelation Use time-series models (ARIMA), add lag features Influential points Investigate data quality, robust regression","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#mini-example-residual-plot","title":"Mini Example (Residual Plot)","text":"Python<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX = np.linspace(1, 9, 9)\ny = np.array([10, 20, 30, 40, 50, 55, 65, 80, 90])\nX_2d = X.reshape(-1, 1)\nmodel = LinearRegression().fit(X_2d, y)\npreds = model.predict(X_2d)\nresid = y - preds\n\nplt.scatter(preds, resid)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\nplt.show()\n</code></pre> <p>Interpreting residuals is as important as the initial accuracy metrics\u2014use diagnostics before trusting model conclusions.</p> <p>Next Deep Dive: An in-repo companion Jupyter notebook <code>notebooks/linear_regression_diagnostics.ipynb</code> includes:</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#interactive-exploration-notebook","title":"Interactive Exploration Notebook","text":"<p>Features provided there: - Full residual diagnostic panel (residuals vs fitted, histogram/KDE, Q-Q, scale-location, leverage vs residuals with Cook's distance, autocorrelation bars). - Statistical assumption tests: Jarque-Bera, Shapiro-Wilk, Breusch-Pagan, Durbin-Watson, Ljung-Box. - Multicollinearity check via Variance Inflation Factor (VIF). - Interactive refit panel (Linear / Ridge / Lasso) with alpha slider &amp; feature selection to observe how diagnostics change. - Figure export helper (<code>save_current_figures()</code>) saving timestamped PNGs under <code>notebooks/reports/</code>. - Residuals export (CSV always; Parquet if optional dependency installed) enriched with leverage + Cook\u2019s distance.</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note6/#enabling-parquet-export","title":"Enabling Parquet Export","text":"<p>Install either <code>pyarrow</code> or <code>fastparquet</code> inside the active virtual environment: Text Only<pre><code>pip install pyarrow\n# or\npip install fastparquet\n</code></pre> Then re-run the export cell (Section 13) in the notebook.</p> <p>Tip: Run cells sequentially the first time; for exploration you can just re-run the interactive cell (Section 11) after adjusting controls.</p>","tags":["regression","supervised-learning","fundamentals","linear-models"]},{"location":"notes/note7/","title":"What is Non-Linear Regression?","text":"<p>Linear Regression assumes a straight-line relationship between input feature(s) and output target. In many real-world scenarios, relationships are curved, saturating, multiplicative, or otherwise non-linear. When a linear (affine) model cannot adequately capture the pattern, we turn to Non-Linear Regression.</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#general-form","title":"General Form","text":"<p>\\(Y = f(X) + \\varepsilon\\) Where \\(f(X)\\) is a non-linear function of the predictors and \\(\\varepsilon\\) is the error term.</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#examples-of-non-linear-relationships","title":"Examples of Non-Linear Relationships","text":"<ul> <li>Exponential growth / decay: Population growth, radioactive decay</li> <li>Polynomial trends: U-shaped or inverted U-shapes (economics, sales over time)</li> <li>Logarithmic: Diminishing returns (learning curves, efficiency vs resources)</li> <li>Logistic (sigmoidal): Growth saturating at a carrying capacity (population, adoption curves)</li> </ul>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#common-non-linear-model-types","title":"Common Non-Linear Model Types","text":"","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#1-polynomial-regression","title":"1. Polynomial Regression","text":"<p>Extends linear regression by adding higher-order terms of features. Though the model is linear in parameters, the relationship between original \\(X\\) and \\(Y\\) becomes non-linear.</p> <p>General (single feature): \\(Y = b_0 + b_1 X + b_2 X^2 + b_3 X^3 + \\dots + b_d X^d + \\varepsilon\\)</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#2-exponential-regression","title":"2. Exponential Regression","text":"<p>Captures multiplicative / exponential growth or decay: \\(Y = a e^{bX}\\) Often log-transformable: \\(\\ln Y = \\ln a + bX\\) when \\(Y&gt;0\\).</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#3-logarithmic-regression","title":"3. Logarithmic Regression","text":"<p>Useful for diminishing returns: \\(Y = a + b \\ln(X)\\)</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#4-logistic-function-s-curve","title":"4. Logistic Function (S-curve)","text":"<p>For bounded growth (continuous) or as the link in Logistic Regression for classification probability modeling: \\(p = \\frac{1}{1 + e^{-(w_0 + w_1 X_1 + \\dots + w_n X_n)}}\\) While logistic regression is typically presented as a classification technique, it is conceptually a non-linear regression on probability space via the sigmoid.</p> <p>Note: Many other non-linear forms exist (power law, Michaelis\u2013Menten, Gompertz, spline-based, kernel methods, neural networks).</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#example-1-polynomial-regression-python","title":"Example 1: Polynomial Regression (Python)","text":"Python<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Sample dataset\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\ny = np.array([2.3, 2.9, 7.6, 15.3, 26.8, 40.5, 61.1, 85.2, 120.3])\n\n# Polynomial features (degree=2)\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n\nplt.scatter(X, y, color='blue')\nplt.plot(X, y_pred, color='red')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Polynomial Regression (Degree 2)\")\nplt.show()\n</code></pre>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#example-2-exponential-regression-curve-fitting","title":"Example 2: Exponential Regression (Curve Fitting)","text":"Python<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\ndef exp_func(x, a, b):\n        return a * np.exp(b * x)\n\nx_data = np.array([0, 1, 2, 3, 4, 5])\ny_data = np.array([2, 3, 7, 20, 55, 148])\n\nparams, _ = curve_fit(exp_func, x_data, y_data)\na, b = params\n\nx_line = np.linspace(0, 5, 100)\ny_line = exp_func(x_line, a, b)\n\nplt.scatter(x_data, y_data, color='blue')\nplt.plot(x_line, y_line, color='red')\nplt.title(\"Exponential Regression Example\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\nprint(f\"Estimated parameters: a={a:.4f}, b={b:.4f}\")\n</code></pre>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#linear-vs-non-linear-regression-comparison","title":"Linear vs Non-Linear Regression (Comparison)","text":"Aspect Linear Regression Non-Linear Regression Model Shape Straight line / hyperplane Curve (polynomial, exponential, logistic, etc.) Equation Form \\(Y = a + bX\\) \\(Y = f(X)\\) (non-linear \\(f\\)) Use Case Simple linear trends Complex / curved patterns Computation Closed-form (OLS) or simple May require iterative curve fitting Interpretability High (coefficients) Varies (may be harder) Risk Underfit curved data Overfit if too flexible","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#practical-notes","title":"Practical Notes","text":"<ul> <li>Start with exploratory plots (scatter + smoothing) to judge linear vs non-linear structure.</li> <li>Polynomial degree should be chosen carefully (use validation or regularization like Ridge/Lasso on polynomial features).</li> <li>For exponential/log models, check domain constraints (\\(Y&gt;0\\), \\(X&gt;0\\) for logs).</li> <li>Consider transformations (log, Box-Cox) before jumping to complex non-linear solvers.</li> <li>For highly flexible unknown forms, splines or tree-based / kernel / neural models may outperform parametric curves.</li> </ul>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note7/#summary","title":"Summary","text":"<ul> <li>Linear Regression only fits straight-line relationships.</li> <li>Non-Linear Regression captures curved, saturating, or multiplicative patterns.</li> <li>Model choice depends on observed data patterns and interpretability needs.</li> </ul> <p>Next: We can explore logistic regression (classification) or regularization of polynomial models if desired.</p>","tags":["regression","non-linear","modeling","fundamentals"]},{"location":"notes/note8/","title":"Model Evaluation in Regression","text":"<p>Evaluating a regression model requires looking at error magnitude and variance explained. No single metric tells the whole story\u2014using a combination helps avoid misleading conclusions.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#1-core-regression-metrics","title":"1. Core Regression Metrics","text":"","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#a-mean-absolute-error-mae","title":"(a) Mean Absolute Error (MAE)","text":"<p>\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\) Average absolute deviation. Interpretable (same units as target). Robust to large outliers relative to squared metrics.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#b-mean-squared-error-mse","title":"(b) Mean Squared Error (MSE)","text":"<p>\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) Squares residuals \u2192 penalizes larger errors more. Smooth gradient (useful for optimization). Sensitive to outliers.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#root-mean-squared-error-rmse","title":"\u00a9 Root Mean Squared Error (RMSE)","text":"<p>\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\) Same units as target. More interpretable than MSE; still emphasizes larger errors.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#d-coefficient-of-determination-r2","title":"(d) Coefficient of Determination (\\(R^2\\))","text":"<p>\\(R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\\) Where: \\(SS_{res} = \\sum (y_i - \\hat{y}_i)^2 \\qquad SS_{tot} = \\sum (y_i - \\bar{y})^2\\) Represents proportion of variance explained. Range roughly: \\((-\\infty, 1]\\). Negative means model is worse than predicting the mean.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#e-adjusted-r2-multiple-regression","title":"(e) Adjusted \\(R^2\\) (Multiple Regression)","text":"<p>\\(R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\) Penalizes addition of non-informative predictors (where \\(p\\) = number of features, \\(n\\) = samples).</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#2-python-example","title":"2. Python Example","text":"Python<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Sample data\nX = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\ny = np.array([1.5, 3.5, 4.2, 5.0, 7.8, 8.5])\n\nmodel = LinearRegression().fit(X, y)\ny_pred = model.predict(X)\n\nmae = mean_absolute_error(y, y_pred)\nmse = mean_squared_error(y, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y, y_pred)\n\nn = len(y)\np = X.shape[1]\nadj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R^2: {r2:.4f}\")\nprint(f\"Adjusted R^2: {adj_r2:.4f}\")\n</code></pre>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#3-when-to-use-which-metric","title":"3. When to Use Which Metric","text":"Metric Use When Strengths Cautions MAE Need interpretability Linear penalty, robust-ish Under-penalizes rare large errors MSE Large errors costly Smooth &amp; differentiable Inflated by outliers RMSE Stakeholders expect natural units Balances frequency &amp; size Still sensitive to outliers \\(R^2\\) Compare baseline vs model Scale-free Misleading with non-linear patterns, can be high with bias Adjusted \\(R^2\\) Feature set comparison Penalizes complexity Not for non-linear/cross-validated model selection","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#4-additional-optional-metrics","title":"4. Additional (Optional) Metrics","text":"<ul> <li>Median Absolute Error (MedAE): Robust to extreme outliers.</li> <li>MAPE: Percentage error; avoid when \\(y_i\\) can be zero or near zero.</li> <li>SMAPE: Symmetric variant of MAPE.</li> <li>\\(R^2\\) on test folds: Use cross-validation for reliable generalization estimate.</li> </ul>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#5-practical-guidance","title":"5. Practical Guidance","text":"<ol> <li>Always inspect residual plots alongside metrics.</li> <li>Report both an absolute error (MAE/RMSE) and a relative/explanatory metric (\\(R^2\\)).</li> <li>Beware high \\(R^2\\) with systematic bias (check residual structure).</li> <li>Use cross-validation for model comparison instead of single train-test splits where feasible.</li> <li>For skewed targets, consider log-transforming before evaluation (then invert carefully when interpreting).</li> </ol>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note8/#6-summary","title":"6. Summary","text":"<ul> <li>MAE / MSE / RMSE quantify average error magnitude.</li> <li>\\(R^2\\) / Adjusted \\(R^2\\) quantify variance explained.</li> <li>No single metric is sufficient\u2014combine them and validate with residual diagnostics.</li> </ul> <p>Next: We can explore error analysis (residual stratification, prediction intervals) or evaluation under distribution shift.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/","title":"Evaluation Metrics in Regression Models","text":"<p>Regression metrics measure how well a model predicts continuous outcomes. They fall broadly into: error magnitude metrics and goodness-of-fit metrics, with some specialized variants for particular data characteristics.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#1-error-based-metrics","title":"1. Error-Based Metrics","text":"","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\) Average absolute deviation; intuitive and in original units. Less sensitive to large outliers than squared metrics.</p> <p>Use when: All errors should contribute proportionally; robust-ish interpretation.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) Squares residuals \u2192 emphasizes large mistakes heavily.</p> <p>Use when: Large deviations are very costly (risk-sensitive domains).</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\) Same units as target; balances interpretability with sensitivity to large errors.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#mean-absolute-percentage-error-mape","title":"Mean Absolute Percentage Error (MAPE)","text":"<p>\\(\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\\) Expresses error as a percentage. Avoid when \\(y_i = 0\\) or near zero, as it explodes.</p> <p>Use when: Stakeholders care about relative error (e.g., demand forecasting).</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#mean-squared-logarithmic-error-msle","title":"Mean Squared Logarithmic Error (MSLE)","text":"<p>\\(\\text{MSLE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1+y_i) - \\log(1+\\hat{y}_i))^2\\) Penalizes underestimation more than overestimation; smooths exponential growth.</p> <p>Use when: Targets grow multiplicatively (population, viral adoption, sales scaling).</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#huber-loss-metric-perspective","title":"Huber Loss (Metric Perspective)","text":"<p>Hybrid of MAE and MSE with threshold \\(\\delta\\): \\(L_\\delta(e) = \\begin{cases} \\frac{1}{2} e^2 &amp; |e| \\le \\delta \\\\ \\delta(|e| - \\frac{1}{2}\\delta) &amp; |e| &gt; \\delta \\end{cases}\\) Less sensitive to outliers than MSE; smoother than pure MAE.</p> <p>Use when: Some outliers exist but complete robustness of MAE not desired.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#2-goodness-of-fit-metrics","title":"2. Goodness-of-Fit Metrics","text":"","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#r2-coefficient-of-determination","title":"\\(R^2\\) (Coefficient of Determination)","text":"<p>\\(R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}, \\quad SS_{res} = \\sum (y_i - \\hat{y}_i)^2, \\quad SS_{tot} = \\sum (y_i - \\bar{y})^2\\) Proportion of variance explained. Negative values mean model underperforms a naive mean predictor.</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#adjusted-r2","title":"Adjusted \\(R^2\\)","text":"<p>\\(R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\) Corrects \\(R^2\\) inflation when adding non-informative predictors (with \\(p\\) features, \\(n\\) samples).</p>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#3-python-example","title":"3. Python Example","text":"Python<pre><code>import numpy as np\nfrom sklearn.metrics import (\n    mean_absolute_error, mean_squared_error, r2_score,\n    mean_absolute_percentage_error, mean_squared_log_error\n)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_regression\n\n# Generate sample regression data\nX, y = make_regression(n_samples=80, n_features=1, noise=12, random_state=42)\nmodel = LinearRegression().fit(X, y)\ny_pred = model.predict(X)\n\nmae = mean_absolute_error(y, y_pred)\nmse = mean_squared_error(y, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y, y_pred)\n\n# Avoid negative or zero values before MSLE / MAPE (example guard)\nif (y &gt; -1).all() and (y_pred &gt; -1).all():\n    try:\n        msle = mean_squared_log_error(y, y_pred)\n    except ValueError:\n        msle = None\nelse:\n    msle = None\n\nmape = mean_absolute_percentage_error(y, y_pred)\n\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R^2: {r2:.4f}\")\nprint(f\"MAPE: {mape:.4f}\")\nprint(f\"MSLE: {msle if msle is not None else 'n/a (invalid for negative values)'}\")\n</code></pre>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#4-choosing-the-right-metric","title":"4. Choosing the Right Metric","text":"Scenario Preferred Metrics Rationale Balanced general performance RMSE + MAE Two perspectives on error magnitude Outliers present MAE / Huber Reduced sensitivity to large errors Large errors very costly MSE / RMSE Squares large residuals Percentage importance MAPE (if no zeros) Relative interpretability Multiplicative growth MSLE Log-scaling stabilizes growth Feature set comparison Adjusted \\(R^2\\) Penalizes complexity","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#5-practical-tips","title":"5. Practical Tips","text":"<ol> <li>Always pair at least one absolute error metric (MAE/RMSE) with a relative/explanatory one (\\(R^2\\) / Adjusted \\(R^2\\)).</li> <li>Inspect residual distributions; metrics can hide structure (see residual diagnostics in earlier notes).</li> <li>Prefer cross-validation averages over single split scores for model selection.</li> <li>For skewed targets, consider log-transform + back-transform carefully when reporting.</li> <li>Avoid MAPE when actual values can be zero or extremely small.</li> </ol>","tags":["regression","evaluation","metrics","fundamentals"]},{"location":"notes/note9/#6-summary","title":"6. Summary","text":"<ul> <li>Error Metrics: MAE, MSE, RMSE, MAPE, MSLE, Huber quantify prediction error specifics.</li> <li>Goodness-of-Fit: \\(R^2\\), Adjusted \\(R^2\\) explain variance captured.</li> <li>No universal best metric\u2014selection depends on domain cost structure and data distribution.</li> </ul> <p>Next: Dive deeper into residual analysis, prediction intervals, or probabilistic regression metrics (Pinball loss, CRPS) if needed.</p>","tags":["regression","evaluation","metrics","fundamentals"]}]}